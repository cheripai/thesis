@article{Medellin,
author = {Medell{\'{i}}n-Azuara, Josu{\'{e}} and Macewan, Duncan and Howitt, Richard E and Sumner, Daniel A and Lund, Jay R and Scheer, Jennifer and Gailey, Robert and Hart, Quinn and Alexander, Nadya D and Arnold, Brad and Kwon, Angela and Bell, Andrew and Li, William},
title = {{Economic Analysis of the 2016 California Drought on Agriculture}},
pages = {1--17},
year = {2016}
}

@article{Lin2001,
author = {Lin, Bin Le and Sakoda, A. and Shibasaki, R. and Suzuki, M.},
doi = {10.1016/S0043-1354(00)00484-X},
isbn = {0043-1354},
issn = {00431354},
journal = {Water Research},
keywords = {Ecosystem model,Fertilisation,Global biogeochemical nitrogen cycle,Leaching load,Nitrate leaching},
number = {8},
pages = {1961--1968},
pmid = {11337842},
title = {{A Modelling Approach to Global Nitrate Leaching Caused by Anthropogenic Fertilisation}},
volume = {35},
year = {2001}
}


@article{Ryan,
author = {Ryan, Larry},
file = {:home/dat/Documents/Papers/Unknown/Ryan - Creating a “Normalized Difference Vegetation Index” (NDVI) image Using MultiSpec.pdf:pdf},
journal = {The GLOBE Program},
mendeley-groups = {Thesis/2,Thesis},
pages = {1--14},
title = {{Creating a “Normalized Difference Vegetation Index” (NDVI) Image Using MultiSpec}},
year = {1997}
}

@article{Liaghat2010,
author = {Liaghat, Shohreh and Balasundram, Siva Kumar},
file = {:home/dat/Documents/Papers/2010/Liaghat, Balasundram - A review The role of remote sensing in precision agriculture.pdf:pdf},
journal = {American Journal of Agricultural {\&} Biological Science},
keywords = {environment,remotely sensed data,spectral reflectance,sustainable agriculture},
mendeley-groups = {Precision Agriculture},
number = {1},
pages = {50--55},
title = {{A Review: The Role of Remote Sensing in Precision Agriculture}},
volume = {5},
year = {2010}
}

@article{Abuleil,
author = {Abuleil, Ammar},
file = {:home/dat/Documents/Papers/2015/Thesis - Mapping Red Clover Uniformity Using Unmanned Aerial Vehicles by.pdf:pdf},
mendeley-groups = {Precision Agriculture/2,Precision Agriculture},
journal = {Conference on Computer and Robot Vision},
title = {{Mapping Red Clover Uniformity Using Unmanned Aerial Vehicles}},
year = {2015}
}

@article{Pan2009,
abstract = {Crop yield is a key element in rural development and an indicator of national food security. A method that could estimate crop yield over large hilly areas would be highly desirable. Methods including high spatial resolution satellite imagery have the potential to achieve this objective. This paper describes a method of integrating QuickBird imagery with a production efficiency model (PEM) to estimate crop yield in Zhonglianchuan, a hilly area on Loess Plateau, China. In the PEM model, crop yield is a function of the photosynthetic active radiation (PAR), fraction of absorbed photosynthetically active radiation (f
                        APAR) and light-use efficiency (LUE). Based on the high spatial resolution QuickBird imagery, a land cover classification is used to attribute a class-specific LUE. The f
                        APAR is related to spectral vegetation indices (SVI), which can be derived from the satellite images. The LUE, f
                        APAR and incident PAR data were combined to estimate the crop yield. Farmer-reported crop yield data in 80 representative plots were used to validate the model output. The results indicated QuickBird imagery can improve the accuracy of predicted results relative to the Landsat TM image. The predicted yield approximated well with the data reported by the farmers (r
                        2 = 0.86; n = 80). The spatial distributions of crop yield derived here also offers valuable information to manage agricultural production and understand ecosystem functioning. ?? 2008.},
author = {Pan, Gang and Sun, Guo Jun and Li, Feng Min},
doi = {10.1016/j.envsoft.2008.09.014},
file = {:home/dat/Documents/Papers/2009/Pan, Sun, Li - Using QuickBird imagery and a production efficiency model to improve crop yield estimation in the semi-arid hilly Loess P.pdf:pdf},
isbn = {1364-8152},
issn = {13648152},
journal = {Environmental Modelling and Software},
keywords = {Chinese Loess Plateau,Crop yield,Production efficiency model,QuickBird imagery,Remote sensing,Semi-arid},
mendeley-groups = {Precision Agriculture},
number = {4},
pages = {510--516},
publisher = {Elsevier Ltd},
title = {{Using QuickBird imagery and a production efficiency model to improve crop yield estimation in the semi-arid hilly Loess Plateau, China}},
url = {http://dx.doi.org/10.1016/j.envsoft.2008.09.014},
volume = {24},
year = {2009}
}

@article{Zhang2012,
abstract = {Precision agriculture (PA) is the application of geospatial techniques and sensors (e.g., geographic information systems, remote sensing, GPS) to identify variations in the field and to deal with them using alternative strategies. In particular, high-resolution satellite imagery is now more commonly used to study these variations for crop and soil conditions. However, the availability and the often prohibitive costs of such imagery would suggest an alternative product for this particular application in PA. Specifically, images taken by low altitude remote sensing platforms, or small unmanned aerial systems (UAS), are shown to be a potential alternative given their low cost of operation in environmental monitoring, high spatial and temporal resolution, and their high flexibility in image acquisition programming. Not surprisingly, there have been several recent studies in the application of UAS imagery for PA. The results of these studies would indicate that, to provide a reliable end product to farmers, advances in platform design, production, standardization of image georeferencing and mosaicing, and information extraction workflow are required. Moreover, it is suggested that such endeavors should involve the farmer, particularly in the process of field design, image acquisition, image interpretation and analysis.},
annote = {Should we be using Leaf Area Index?
Convert brightness value from camera to reflectance?},
author = {Zhang, Chunhua and Kovacs, John M.},
doi = {10.1007/s11119-012-9274-5},
file = {:home/dat/Documents/Papers/2012/Zhang, Kovacs - The application of small unmanned aerial systems for precision agriculture A review.pdf:pdf},
isbn = {1385-2256},
issn = {13852256},
journal = {Precision Agriculture},
keywords = {Farmer participation,Low altitude remote sensing,UAS aviation regulations,UAS cameras,UAS limitations,UAS platforms,Unmanned aerial systems (UAS)},
mendeley-groups = {Precision Agriculture},
number = {6},
pages = {693--712},
pmid = {83171190},
title = {{The application of small unmanned aerial systems for precision agriculture: a review}},
volume = {13},
year = {2012}
}

@article{Nature_DL,
abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
archivePrefix = {arXiv},
arxivId = {arXiv:1312.6184v5},
author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
doi = {10.1038/nature14539},
eprint = {arXiv:1312.6184v5},
file = {:home/dat/Documents/Papers/2015/LeCun et al. - Deep learning.pdf:pdf},
isbn = {3135786504},
issn = {0028-0836},
journal = {Nature},
mendeley-groups = {Deep Learning Roadmap},
number = {7553},
pages = {436--444},
pmid = {26017442},
title = {{Deep learning}},
volume = {521},
year = {2015}
}

@book{Murphy,
archivePrefix = {arXiv},
arxivId = {0-387-31073-8},
author = {{P. Murphy}, Kevin},
booktitle = {Machine Learning: A Probabilistic Perspective},
doi = {10.1007/SpringerReference_35834},
eprint = {0-387-31073-8},
file = {:home/dat/Documents/Papers/2012/P. Murphy - Machine Learning A Probabilistic Perspective.pdf:pdf},
isbn = {9780262018029},
issn = {0262018020},
mendeley-groups = {Books},
pmid = {20236947},
title = {{Machine Learning: A Probabilistic Perspective}},
year = {2012},
publisher = {The MIT Press}
}

@book{Bishop2007,
archivePrefix = {arXiv},
arxivId = {0-387-31073-8},
author = {Bishop, C. M.},
booktitle = {Journal of Electronic Imaging},
doi = {10.1117/1.2819119},
eprint = {0-387-31073-8},
file = {:home/dat/Documents/Papers/2007/Bishop - Pattern Recognition and Machine Learning.pdf:pdf},
isbn = {0387310738},
issn = {1017-9909},
mendeley-groups = {Books},
pmid = {8943268},
title = {{Pattern Recognition and Machine Learning}},
year = {2007},
publisher = {Springer}
}

@article{vgg,
abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
archivePrefix = {arXiv},
arxivId = {1409.1556},
author = {Simonyan, Karen and Zisserman, Andrew},
doi = {10.1016/j.infsof.2008.09.005},
eprint = {1409.1556},
file = {:home/dat/Documents/Papers/2015/Simonyan, Zisserman - Very Deep Convolutional Networks for Large-Scale Image Recognition.pdf:pdf},
isbn = {9781450341448},
issn = {09505849},
journal = {International Conference on Learning Representations (ICRL)},
mendeley-groups = {cs664},
pages = {1--14},
pmid = {16873662},
title = {{Very Deep Convolutional Networks for Large-Scale Image Recognition}},
url = {http://arxiv.org/abs/1409.1556},
year = {2015}
}

@article{resnet,
abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learn- ing residual functions with reference to the layer inputs, in- stead of learning unreferenced functions. We provide com- prehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers—8× deeper than VGG nets [41] but still having lower complex- ity. An ensemble of these residual nets achieves 3.57{\%} error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our ex- tremely deep representations, we obtain a 28{\%} relative im- provement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC {\&} COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet local- ization, COCO detection, and COCO segmentation.},
archivePrefix = {arXiv},
arxivId = {1512.03385},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
doi = {10.3389/fpsyg.2013.00124},
eprint = {1512.03385},
file = {:home/dat/Documents/Papers/2015/He et al. - Deep Residual Learning for Image Recognition.pdf:pdf},
isbn = {978-1-4673-6964-0},
issn = {1664-1078},
journal = {Arxiv.Org},
keywords = {deep learning,denoising auto-encoder,image denoising},
mendeley-groups = {cs664},
number = {3},
pages = {171--180},
pmid = {23554596},
title = {{Deep Residual Learning for Image Recognition}},
url = {http://arxiv.org/pdf/1512.03385v1.pdf},
volume = {7},
year = {2015}
}

@article{inception,
abstract = {Convolutional networks are at the core of most state-of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we are exploring ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible. We benchmark our methods on the ILSVRC 2012 classification challenge validation set and demonstrate substantial gains over the state of the art via to carefully factorized convolutions and aggressive regularization: 21.2{\%} top-1 and 5.6{\%} top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters.},
archivePrefix = {arXiv},
arxivId = {1512.00567},
author = {Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jonathon and Wojna, Zbigniew},
doi = {10.1002/2014GB005021},
eprint = {1512.00567},
file = {:home/dat/Documents/Papers/2016/Szegedy et al. - Rethinking the Inception Architecture for Computer Vision.pdf:pdf},
isbn = {9781617796029},
issn = {08866236},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR)},
mendeley-groups = {cs664},
pages = {2818--2826},
pmid = {8190083},
title = {{Rethinking the Inception Architecture for Computer Vision}},
url = {http://arxiv.org/abs/1512.00567},
year = {2016}
}

@article{adam,
abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
archivePrefix = {arXiv},
arxivId = {1412.6980},
author = {Kingma, Diederik and Ba, Jimmy},
eprint = {1412.6980},
file = {:home/dat/Documents/Papers/2014/Kingma, Ba - Adam A Method for Stochastic Optimization.pdf:pdf},
journal = {International Conference on Learning Representations},
mendeley-groups = {cs664},
pages = {1--13},
title = {{Adam: A Method for Stochastic Optimization}},
url = {http://arxiv.org/abs/1412.6980},
year = {2014}
}

@article{batchnorm,
abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parame- ters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phe- nomenon as internal covariate shift, and ad- dress the problem by normalizing layer inputs. Our method draws its strength from making nor- malization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less care- ful about initialization, and in some cases elim- inates the need for Dropout. Applied to a state- of-the-art image classification model, Batch Nor- malization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensem- ble of batch-normalized networks, we improve upon the best published result on ImageNet clas- sification: reaching 4.82{\%} top-5 test error, ex- ceeding the accuracy of human raters.},
archivePrefix = {arXiv},
arxivId = {abs/1502.03167},
author = {Ioffe, Sergey and Szegedy, Christian},
eprint = {1502.03167},
file = {:home/dat/Documents/Papers/2015/Ioffe, Szegedy - Batch Normalization Accelerating Deep Network Training by Reducing Internal Covariate Shift.pdf:pdf},
journal = {CoRR},
mendeley-groups = {cs664},
primaryClass = {abs},
title = {{Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift}},
url = {http://arxiv.org/abs/1502.03167},
year = {2015}
}

@article{dropout,
abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different " thinned " networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
archivePrefix = {arXiv},
arxivId = {1102.4807},
author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
doi = {10.1214/12-AOS1000},
eprint = {1102.4807},
file = {:home/dat/Documents/Papers/2014/Srivastava et al. - Dropout A Simple Way to Prevent Neural Networks from Overfitting.pdf:pdf},
isbn = {1532-4435},
issn = {15337928},
journal = {Journal of Machine Learning Research},
keywords = {deep learning,model combination,neural networks,regularization},
pages = {1929--1958},
title = {{Dropout: A Simple Way to Prevent Neural Networks from Overfitting}},
volume = {15},
year = {2014}
}

@article{Lettuce,
author = {Geisseler, Daniel and Horwath, William R},
file = {:home/dat/Documents/Papers/2016/Geisseler, Horwath - Lettuce Production in California.pdf:pdf},
mendeley-groups = {Precision Agriculture},
pages = {1--4},
title = {{Lettuce Production in California}},
year = {2016}
}

@article{Orange,
author = {Geisseler, Daniel and Horwath, William R},
file = {:home/dat/Documents/Papers/2016/Geisseler, Horwath - Citrus Production in California.pdf:pdf},
pages = {1--4},
title = {{Citrus Production in California}},
year = {2016}
}

@article{r-cnn,
abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet [7] and Fast R-CNN [5] have reduced the running time of these detection networks, exposing region pro-posal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully-convolutional network that simultaneously predicts object bounds and objectness scores at each position. RPNs are trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. With a simple alternating optimization, RPN and Fast R-CNN can be trained to share convolu-tional features. For the very deep VGG-16 model [18], our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007 (73.2{\%} mAP) and 2012 (70.4{\%} mAP) using 300 proposals per image. The code will be released.},
archivePrefix = {arXiv},
arxivId = {arXiv:1506.01497v1},
author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
doi = {10.1016/j.nima.2015.05.028},
eprint = {arXiv:1506.01497v1},
file = {:home/dat/Documents/Papers/2015/Ren et al. - Faster R-CNN Towards Real-Time Object Detection with Region Proposal Networks.pdf:pdf},
isbn = {0162-8828 VO - PP},
issn = {01689002},
journal = {Nips},
pages = {1--10},
pmid = {27295650},
title = {{Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks}},
year = {2015}
}

@article{ssd,
abstract = {We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. Our SSD model is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stage and encapsulates all computation in a single network. This makes SSD easy to train and straightforward to integrate into systems that require a detection component. Experimental results on the PASCAL VOC, MS COCO, and ILSVRC datasets confirm that SSD has comparable accuracy to methods that utilize an additional object proposal step and is much faster, while providing a unified framework for both training and inference. Compared to other single stage methods, SSD has much better accuracy, even with a smaller input image size. For {\$}300\backslashtimes 300{\$} input, SSD achieves 72.1{\%} mAP on VOC2007 test at 58 FPS on a Nvidia Titan X and for {\$}500\backslashtimes 500{\$} input, SSD achieves 75.1{\%} mAP, outperforming a comparable state of the art Faster R-CNN model. Code is available at https://github.com/weiliu89/caffe/tree/ssd .},
archivePrefix = {arXiv},
arxivId = {1512.02325},
author = {Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy, Christian and Reed, Scott and Fu, Cheng Yang and Berg, Alexander C.},
doi = {10.1007/978-3-319-46448-0_2},
eprint = {1512.02325},
file = {:home/dat/Documents/Papers/2016/Liu et al. - SSD Single shot multibox detector.pdf:pdf},
isbn = {9783319464473},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Convolutional neural network,Real-time object detection},
pages = {21--37},
pmid = {23739795},
title = {{SSD: Single shot multibox detector}},
volume = {9905 LNCS},
year = {2016}
}

@article{densenet,
abstract = {Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections - one between each layer and its subsequent layer - our network has L(L+1)/2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less memory and computation to achieve high performance. Code and models are available at https://github.com/liuzhuang13/DenseNet .},
archivePrefix = {arXiv},
arxivId = {1608.06993},
author = {Huang, Gao and Liu, Zhuang and Weinberger, Kilian Q. and van der Maaten, Laurens},
doi = {10.1109/CVPR.2017.243},
eprint = {1608.06993},
file = {:home/dat/Documents/Papers/2016/Huang et al. - Densely Connected Convolutional Networks.pdf:pdf},
issn = {0002-9645},
pmid = {211888},
title = {{Densely Connected Convolutional Networks}},
url = {http://arxiv.org/abs/1608.06993},
year = {2016}
}

@article{fc-densenet,
abstract = {State-of-the-art approaches for semantic image segmentation are built on Convolutional Neural Networks (CNNs). The typical segmentation architecture is composed of (a) a downsampling path responsible for extracting coarse semantic features, followed by (b) an upsampling path trained to recover the input image resolution at the output of the model and, optionally, (c) a post-processing module (e.g. Conditional Random Fields) to refine the model predictions. Recently, a new CNN architecture, Densely Connected Convolutional Networks (DenseNets), has shown excellent results on image classification tasks. The idea of DenseNets is based on the observation that if each layer is directly connected to every other layer in a feed-forward fashion then the network will be more accurate and easier to train. In this paper, we extend DenseNets to deal with the problem of semantic segmentation. We achieve state-of-the-art results on urban scene benchmark datasets such as CamVid and Gatech, without any further post-processing module nor pretraining. Moreover, due to smart construction of the model, our approach has much less parameters than currently published best entries for these datasets. Code to reproduce the experiments is available here : https://github.com/SimJeg/FC-DenseNet/blob/master/train.py},
archivePrefix = {arXiv},
arxivId = {1611.09326},
author = {Jegou, Simon and Drozdzal, Michal and Vazquez, David and Romero, Adriana and Bengio, Yoshua},
doi = {10.1109/CVPRW.2017.156},
eprint = {1611.09326},
file = {:home/dat/Documents/Papers/2017/Jegou et al. - The One Hundred Layers Tiramisu Fully Convolutional DenseNets for Semantic Segmentation.pdf:pdf},
isbn = {9781538607336},
issn = {21607516},
journal = {IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
pages = {1175-1183},
pmid = {19244017},
title = {{The One Hundred Layers Tiramisu: Fully Convolutional DenseNets for Semantic Segmentation}},
volume = {2017-July},
year = {2017}
}
